---
title: "Are Word Embedding Methods Stable and Should We Care About It?"
authors: "Angana Borah, Manash Pratim Barman, Amit C Awekar"
collection: publications
permalink: /publication/2021-08-29-HT21
excerpt: 'The central idea of this paper is to explore the stability measurement of WEMs using intrinsic evaluation based on word similarity and observe the effect of stability on two downstream tasks like Clustering and Fairness evaluation.'
date: 2021-08-29
venue: 'ACM HyperText'
paperurl: <!-- '[http://academicpages.github.io/files/paper1.pdf](https://dl.acm.org/doi/10.1145/3465336.3475098)' -->
citation: 'Angana Borah, Manash Pratim Barman, Amit C Awekar. (2021). &quot;Are Word Embedding Methods Stable and Should We Care About It?&quot; <i>In Proceedings of the 32nd ACM Conference on Hypertext and Social Media </i>. (pp. 45-55).'
---
<!-- This paper is about the number 2. The number 3 is left for future work. -->

[Download paper here](https://dl.acm.org/doi/10.1145/3465336.3475098)

<!-- Recommended citation: Angana Borah, Manash Pratim Barman, Amit C Awekar. (2021). "Are Word Embedding Methods Stable and Should We Care About It?" <i>In Proceedings of the 32nd ACM Conference on Hypertext and Social Media </i>. (pp. 45-55). -->


The central idea of this paper is to explore the stability measurement of WEMs using intrinsic evaluation based on word similarity. We experiment with three popular WEMs: Word2Vec, GloVe, and fastText. For stability measurement, we investigate the effect of five parameters involved in training these models. We perform experiments using four real-world datasets from different domains: Wikipedia, News, Song lyrics, and European parliament proceedings. We also observe the effect of WEM stability on two downstream tasks: Clustering and Fairness evaluation. Our experiments indicate that amongst the three WEMs, fastText is the most stable, followed by GloVe and Word2Vec.
